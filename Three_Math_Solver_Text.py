import os
import json
import time
import pandas as pd
from openai import OpenAI
import re
import concurrent.futures
from threading import Lock
import threading
import base64
from io import BytesIO
from urllib.parse import urlparse
from pathlib import Path
from PIL import Image


# çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤å…±äº«èµ„æº
print_lock = Lock()

# å¸¸ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆä»…ä¾›å‚è€ƒï¼‰
COMMON_MODELS = [
    "Qwen3-235B-A22B",
    "gpt-4o-0806-global",
    "gpt-4o-mini-0718-global",
    "claude-3-5-sonnet-20241022",
    "DeepSeek-R1-671B",
    "o1-preview-0912-global",
    "o1-mini-0912-global",
    "gpt-4-turbo",
    "claude-3-opus",
    "gemini-pro",
    "qwq-32b"
]

def _extract_local_image_path(image_url: str, folder: str = "image") -> str:
    """æŠŠ .../image/000001.jpg?xxx â†’ image/000001.jpgï¼ˆè‹¥æ–‡ä»¶å­˜åœ¨æ‰è¿”å›ï¼‰"""
    if not image_url:
        return ""
    try:
        parsed = urlparse(str(image_url).strip())
        filename = Path(parsed.path).name
        if not filename:
            return ""
        candidate = os.path.join(folder, filename)
        return candidate if os.path.exists(candidate) else ""
    except Exception:
        return ""

def _build_data_url_with_pillow(img_path: str, max_size=(2048, 2048)) -> str:
    """Pillow æ‰“å¼€æœ¬åœ°å›¾ï¼Œå¿…è¦æ—¶ç­‰æ¯”ç¼©å° â†’ dataURL(Base64)"""
    with Image.open(img_path) as im:
        if im.mode not in ("RGB", "RGBA", "L"):
            im = im.convert("RGB")
        w, h = im.size
        if w > max_size[0] or h > max_size[1]:
            r = min(max_size[0] / w, max_size[1] / h)
            im = im.resize((int(w*r), int(h*r)), Image.Resampling.LANCZOS)

        fmt = (im.format or "JPEG").upper()
        if fmt not in ("PNG", "JPEG", "WEBP"):
            fmt = "JPEG"

        buf = BytesIO()
        if fmt == "PNG":
            im.save(buf, format="PNG", optimize=True)
            mime = "image/png"
        elif fmt == "WEBP":
            im.save(buf, format="WEBP", method=6)
            mime = "image/webp"
        else:
            im.save(buf, format="JPEG", quality=95, optimize=True)
            mime = "image/jpeg"

        b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
        return f"data:{mime};base64,{b64}"
    
def get_openai_client():
    """ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„OpenAIå®¢æˆ·ç«¯"""
    return OpenAI(
        api_key="sk-7R6saeXf7d6fGdTOI9hUiB5q5aXKIRSZwpU5SgR9i2x58Ljo",
        base_url="https://api.302.ai/v1",
    )


def safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
    with print_lock:
        thread_id = threading.current_thread().ident
        print(f"[Thread-{thread_id}]", *args, **kwargs)


def retry_api_call(func, max_retries=2, retry_delay=3, *args, **kwargs):
    """APIè°ƒç”¨é‡è¯•æœºåˆ¶"""
    last_exception = None

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                safe_print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•è°ƒç”¨API...")
                time.sleep(retry_delay)

            result = func(*args, **kwargs)

            if attempt > 0:
                safe_print(f"APIè°ƒç”¨åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•åæˆåŠŸ")

            return result

        except Exception as e:
            last_exception = e
            error_msg = str(e).lower()

            retryable_errors = [
                'rate limit', 'timeout', 'connection', 'server error',
                'busy', 'overloaded', 'unavailable', '429', '500',
                '502', '503', '504', 'æ¨¡å‹ç¹å¿™', 'å“åº”å†…å®¹è¿‡çŸ­',
                'æ¨¡å‹æ— æ³•ç”Ÿæˆç­”æ¡ˆ', 'è¯·ç¨åé‡è¯•', 'Engine concurrency conflict!'
            ]

            is_retryable = any(error in error_msg for error in retryable_errors)

            if attempt < max_retries and is_retryable:
                safe_print(f"APIè°ƒç”¨å¤±è´¥ (ç¬¬ {attempt + 1} æ¬¡): {e}")
                safe_print(f"è¿™æ˜¯å¯é‡è¯•çš„é”™è¯¯ï¼Œå°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
            elif attempt < max_retries:
                safe_print(f"APIè°ƒç”¨å¤±è´¥ (ç¬¬ {attempt + 1} æ¬¡): {e}")
                safe_print(f"è¿™å¯èƒ½ä¸æ˜¯ç½‘ç»œé—®é¢˜ï¼Œä½†ä»å°†é‡è¯•...")
            else:
                safe_print(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {e}")
                break

    raise last_exception


def compare_answers_with_gpt4o(standard_answer, model_answer, question_text, attempt_num, question_num):
    """ä½¿ç”¨GPT-5æ¯”å¯¹ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
    if not standard_answer or not model_answer or model_answer in ["[ç©º]", "[è§£ç­”ä¸å®Œæ•´]"]:
        return "æ— æ³•æ¯”å¯¹"

    client = get_openai_client()

    safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¼€å§‹ä½¿ç”¨GPT-4oæ¯”å¯¹ç­”æ¡ˆ...")

    compare_prompt = f"""è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªç­”æ¡ˆæ˜¯å¦ç›¸ç­‰æˆ–ç­‰ä»·ï¼š

é¢˜ç›®ï¼š{question_text}

æ ‡å‡†ç­”æ¡ˆï¼š{standard_answer}
æ¨¡å‹ç­”æ¡ˆï¼š{model_answer}

æ¯”è¾ƒè¦æ±‚ï¼š
1. åˆ¤æ–­ä¸¤ä¸ªæ•°å­¦ç­”æ¡ˆåœ¨æ˜¯å¦ç­‰ä»·
2. è€ƒè™‘ä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼ˆå¦‚ï¼š2âˆš3 å’Œ 2*sqrt(3)ï¼Œ1/2 å’Œ 0.5ï¼‰
3. è€ƒè™‘ç­”æ¡ˆé¡ºåºï¼ˆå¦‚æœæ˜¯å¤šä¸ªç­”æ¡ˆï¼‰
4. å¿½ç•¥æ ¼å¼å·®å¼‚ï¼Œå…³æ³¨æœ¬è´¨

è¯·åªå›ç­”ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š
- "æ­£ç¡®" - å¦‚æœä¸¤ä¸ªç­”æ¡ˆæ•°å­¦ä¸Šç­‰ä»·
- "é”™è¯¯" - å¦‚æœä¸¤ä¸ªç­”æ¡ˆæ•°å­¦ä¸Šä¸ç­‰ä»·
- "æ— æ³•åˆ¤æ–­" - å¦‚æœæ— æ³•ç¡®å®šæ˜¯å¦ç­‰ä»·

å›ç­”ï¼š"""

    try:
        compare_response = client.chat.completions.create(
            model="qwen3-max-2025-09-23",
            messages=[{"role": "user", "content": compare_prompt}],
            temperature=0,
            max_tokens=50
        )

        result = compare_response.choices[0].message.content.strip()
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - GPT-4oæ¯”å¯¹ç»“æœ: {result}")

        # æ ‡å‡†åŒ–ç»“æœ
        if "æ­£ç¡®" in result:
            return "æ­£ç¡®"
        elif "é”™è¯¯" in result:
            return "é”™è¯¯"
        else:
            return "æ— æ³•åˆ¤æ–­"

    except Exception as e:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - GPT-4oæ¯”å¯¹å¤±è´¥: {e}")
        return "æ¯”å¯¹å¤±è´¥"


def call_model_with_retry(question, model_name, attempt_num, question_num):
    """è°ƒç”¨æŒ‡å®šæ¨¡å‹çš„å‡½æ•°ï¼Œä¿®å¤æ·±åº¦æ€è€ƒé—®é¢˜"""
    client = get_openai_client()
    thread_id = threading.current_thread().ident

    # æ„é€ æ•°å­¦é—®é¢˜æç¤º
    math_prompt = f"""è¯·è§£å†³ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨\\boxed{{}}ä¸­ï¼š

{question}

è¯·æä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤ï¼Œå¹¶åœ¨æœ€åç”¨\\boxed{{}}æ ‡å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"""

    start_time = time.time()
    safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¼€å§‹è°ƒç”¨{model_name}æ¨¡å‹...")

    full_response = ""
    thinking_content = ""

    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„è°ƒç”¨æ–¹å¼
    if "o1" in model_name.lower():
        # o1ç³»åˆ—æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": math_prompt}],
        )
        full_response = response.choices[0].message.content
        thinking_content = ""

    elif "qwq" in model_name.lower():
        # QwQæ¨¡å‹åªæ”¯æŒæµå¼æ¨¡å¼
        try:
            safe_print(f"ä½¿ç”¨QwQæµå¼æ¨¡å¼...")
            stream = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": math_prompt}],
                stream=True,
                # temperature=0.7,
            )

            full_response = ""
            thinking_content = ""

            for chunk in stream:
                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta

                # å¤„ç†æ­£å¸¸å›å¤å†…å®¹
                if hasattr(delta, "content") and delta.content:
                    full_response += delta.content

            safe_print(f"QwQæµå¼æ¨¡å¼å®Œæˆ - å›å¤å†…å®¹: {len(full_response)}å­—ç¬¦")

        except Exception as e:
            safe_print(f"QwQæµå¼æ¨¡å¼å¤±è´¥: {e}")
            raise e

    elif "qwen" in model_name.lower():
        # Qwenæ¨¡å‹æ”¯æŒæµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
        try:
            safe_print(f"ä½¿ç”¨Qwenæ·±åº¦æ€è€ƒæ¨¡å¼...")
            stream = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": math_prompt}],
                stream=True,
                extra_body={"enable_thinking": False}  # åƒé—®æ¨¡å‹çš„æ€è€ƒå‚æ•°
            )

            full_response = ""
            thinking_content = ""
            is_answering = False

            for chunk in stream:
                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta

                # å¤„ç†æ€è€ƒå†…å®¹ - åƒé—®æ¨¡å‹ä½¿ç”¨ reasoning_content å­—æ®µ
                if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                    thinking_content += delta.reasoning_content
                    if not is_answering:
                        is_answering = False  # è¿˜åœ¨æ€è€ƒé˜¶æ®µ

                # å¤„ç†æ­£å¸¸å›å¤å†…å®¹
                if hasattr(delta, "content") and delta.content:
                    if not is_answering:
                        safe_print(f"å¼€å§‹ç”Ÿæˆå›å¤ï¼Œæ€è€ƒè¿‡ç¨‹æ€»é•¿åº¦: {len(thinking_content)}")
                        is_answering = True
                    full_response += delta.content

            safe_print(f"Qwenæ€è€ƒæ¨¡å¼å®Œæˆ - æ€è€ƒå†…å®¹: {len(thinking_content)}å­—ç¬¦, å›å¤å†…å®¹: {len(full_response)}å­—ç¬¦")

        except Exception as e:
            # å¦‚æœæ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæµå¼æ¨¡å¼
            safe_print(f"Qwenæ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæµå¼æ¨¡å¼: {e}")
            stream = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": math_prompt}],
                stream=True,
            )

            full_response = ""
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content

            thinking_content = ""

    elif "deepseek-r1" in model_name.lower():
        # DeepSeek-R1æ¨¡å‹æ”¯æŒæµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼
        try:
            safe_print(f"ä½¿ç”¨DeepSeek-R1æ·±åº¦æ€è€ƒæ¨¡å¼...")
            stream = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": math_prompt}],
                stream=True,
            )

            full_response = ""
            thinking_content = ""
            is_answering = False

            for chunk in stream:
                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta

                # å¤„ç†æ€è€ƒå†…å®¹ - DeepSeek-R1å¯èƒ½ä½¿ç”¨ä¸åŒçš„å­—æ®µå
                thinking_fields = ['reasoning_content', 'thinking_content', 'thought_content']
                for field in thinking_fields:
                    if hasattr(delta, field) and getattr(delta, field) is not None:
                        thinking_content += getattr(delta, field)
                        if not is_answering:
                            is_answering = False  # è¿˜åœ¨æ€è€ƒé˜¶æ®µ
                        break

                # å¤„ç†æ­£å¸¸å›å¤å†…å®¹
                if hasattr(delta, "content") and delta.content:
                    if not is_answering:
                        safe_print(f"å¼€å§‹ç”Ÿæˆå›å¤ï¼Œæ€è€ƒè¿‡ç¨‹æ€»é•¿åº¦: {len(thinking_content)}")
                        is_answering = True
                    full_response += delta.content

            safe_print(
                f"DeepSeek-R1æ€è€ƒæ¨¡å¼å®Œæˆ - æ€è€ƒå†…å®¹: {len(thinking_content)}å­—ç¬¦, å›å¤å†…å®¹: {len(full_response)}å­—ç¬¦")

        except Exception as e:
            # å¦‚æœæ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæµå¼æ¨¡å¼
            safe_print(f"DeepSeek-R1æ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæµå¼æ¨¡å¼: {e}")
            try:
                stream = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": math_prompt}],
                    stream=True,
                )

                full_response = ""
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content

                thinking_content = ""
                safe_print(f"DeepSeek-R1æ™®é€šæµå¼æ¨¡å¼å®Œæˆ - å›å¤å†…å®¹: {len(full_response)}å­—ç¬¦")

            except Exception as e2:
                # å¦‚æœæµå¼ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨éæµå¼æ¨¡å¼
                safe_print(f"DeepSeek-R1æµå¼æ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨éæµå¼æ¨¡å¼: {e2}")
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": math_prompt}],
                )
                full_response = response.choices[0].message.content
                thinking_content = ""

    else:
        # å…¶ä»–æ¨¡å‹çš„æ ‡å‡†è°ƒç”¨
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": math_prompt}],
        )
        full_response = response.choices[0].message.content
        thinking_content = ""

    end_time = time.time()
    duration = end_time - start_time

    # æ£€æµ‹å“åº”æ—¶é—´è¿‡çŸ­å’Œé”™è¯¯å“åº”
    if duration < 2:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - è­¦å‘Šï¼šè°ƒç”¨æ—¶é—´è¿‡çŸ­({duration:.2f}ç§’)")

    if len(full_response.strip()) < 50:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - è­¦å‘Šï¼šå“åº”å†…å®¹è¿‡çŸ­({len(full_response)}å­—ç¬¦)")
        raise Exception(f"å“åº”å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯APIè°ƒç”¨å¼‚å¸¸")

    # æ£€æµ‹é”™è¯¯å“åº”å†…å®¹
    error_keywords = [
        "concurrency conflict", "engine", "error", "failed",
        "busy", "unavailable", "limit", "quota", "è¯·ç¨åé‡è¯•"
    ]

    if any(keyword in full_response.lower() for keyword in error_keywords):
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - æ£€æµ‹åˆ°é”™è¯¯å“åº”: {full_response[:100]}...")
        raise Exception(f"æ¨¡å‹è¿”å›é”™è¯¯ä¿¡æ¯: {full_response[:100]}")

    safe_print(
        f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - {model_name}è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’ï¼Œå“åº”é•¿åº¦: {len(full_response)}å­—ç¬¦ï¼Œæ€è€ƒé•¿åº¦: {len(thinking_content)}å­—ç¬¦")

    return {
        'full_response': full_response.strip(),
        'thinking_content': thinking_content,
        'duration': duration,
        'model_name': model_name
    }


def call_gpt4o_with_retry(response, question_text, attempt_num, question_num):
    """ä½¿ç”¨å¤§æ¨¡å‹æå–ç­”æ¡ˆ"""
    client = get_openai_client()

    if len(response.strip()) < 50:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - è­¦å‘Šï¼šè¾“å…¥çš„è§£é¢˜è¿‡ç¨‹è¿‡çŸ­")

    safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¼€å§‹è°ƒç”¨GPT-4oæå–ç­”æ¡ˆ...")

    extract_prompt = f"""ä»ä»¥ä¸‹æ•°å­¦è§£ç­”ä¸­æå–æœ€ç»ˆç­”æ¡ˆï¼š

{response}

è¦æ±‚ï¼š
1. æ‰¾åˆ° \\boxed{{}} ä¸­çš„å†…å®¹ï¼Œæå–å…¶ä¸­çš„æ•°å­¦è¡¨è¾¾å¼
2. å¦‚æœæ²¡æœ‰ \\boxed{{}}ï¼Œæå–æœ€ç»ˆçš„æ•°å€¼ç­”æ¡ˆæˆ–ç»“è®º
3. å»æ‰æ‰€æœ‰æ ¼å¼ç¬¦å·ï¼š\\boxed{{}}ã€$$ã€\\(\\)ç­‰
4. å¦‚æœæœ‰å¤šä¸ªç­”æ¡ˆï¼Œç”¨é€—å·åˆ†éš”åœ¨ä¸€è¡Œå†…
5. ä¿æŒæ•°å­¦ç¬¦å·å¦‚ \\sqrt{{}}ã€\\frac{{}}ç­‰

åªè¿”å›çº¯å‡€çš„æ•°å­¦ç­”æ¡ˆï¼Œä¸è¦ä»»ä½•æ ¼å¼åŒ…è£…ã€‚

ç­”æ¡ˆï¼š"""

    extract_response = client.chat.completions.create(
        model="qwen3-max-2025-09-23",
        messages=[{"role": "user", "content": extract_prompt}],
        temperature=0,
        max_tokens=200
    )

    result = extract_response.choices[0].message.content.strip()
    safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - GPT-5æå–ç­”æ¡ˆå®Œæˆ: {result}")

    return result


def extract_answer_with_gpt4o(response, question_text, attempt_num, question_num):
    """ä½¿ç”¨å¤§æ¨¡å‹å–ç­”æ¡ˆ"""
    if not response:
        return ""

    try:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - ä½¿ç”¨å¤§æ¨¡å‹æå–ç­”æ¡ˆ...")
        extracted_answer = retry_api_call(
            call_gpt4o_with_retry,
            max_retries=2,
            retry_delay=3,
            response=response,
            question_text=question_text,
            attempt_num=attempt_num,
            question_num=question_num
        )

        if "è§£ç­”ä¸å®Œæ•´" in extracted_answer:
            safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¤§æ¨¡å‹æ£€æµ‹åˆ°è§£ç­”ä¸å®Œæ•´")
            return "[è§£ç­”ä¸å®Œæ•´]"
        elif "æ— æ³•ç¡®å®š" in extracted_answer:
            safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¤§æ¨¡å‹æ— æ³•ç¡®å®šç­”æ¡ˆ")
            return ""

        cleaned_answer = extracted_answer.strip()
        return cleaned_answer if cleaned_answer else ""

    except Exception as e:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¤§æ¨¡å‹æå–ç­”æ¡ˆå¤±è´¥: {e}")
        return ""

# === [REPLACE] å•æ¬¡è¯¢é—®ï¼šè‡ªåŠ¨æ ¹æ®æ˜¯å¦å«å›¾ & æ¨¡å‹æ˜¯å¦æ”¯æŒvision é€‰æ‹©è°ƒç”¨æ–¹å¼ ===
def ask_model_for_math_single(question, model_name, attempt_num, question_num, standard_answer="", image_path=None):
    thread_id = threading.current_thread().ident
    has_image = bool(image_path) and os.path.exists(image_path)
    use_vision = has_image

    mode_tag = "VISION" if use_vision else "TEXT"
    safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - å¼€å§‹å¤„ç†({model_name}, {mode_tag})...")

    try:
        if use_vision:
            result = retry_api_call(
                call_model_with_retry_vision,
                max_retries=2,
                retry_delay=5,
                question=question,
                model_name=model_name,
                attempt_num=attempt_num,
                question_num=question_num,
                image_path=image_path
            )
        else:
            result = retry_api_call(
                call_model_with_retry,
                max_retries=2,
                retry_delay=5,
                question=question,
                model_name=model_name,
                attempt_num=attempt_num,
                question_num=question_num
            )

        full_response = result['full_response']
        thinking_content = result.get('thinking_content', "")
        duration = result['duration']

        # æå–æœ€ç»ˆç­”æ¡ˆ
        answer = extract_answer_with_gpt4o(full_response, question, attempt_num, question_num)

        # æ¯”å¯¹
        comparison_result = "æ— æ ‡å‡†ç­”æ¡ˆ"
        if standard_answer and standard_answer.strip():
            comparison_result = retry_api_call(
                compare_answers_with_gpt4o,
                max_retries=1,
                retry_delay=2,
                standard_answer=standard_answer,
                model_answer=answer,
                question_text=question,
                attempt_num=attempt_num,
                question_num=question_num
            )

        ans_display = answer if answer else "[ç©º]"
        safe_print(
            f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡æ±‚è§£å®Œæˆ({model_name},{mode_tag}) - è€—æ—¶: {duration:.2f}s - ç­”æ¡ˆ: {ans_display} - æ¯”å¯¹: {comparison_result}")

        if thinking_content:
            safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - æ€è€ƒè¿‡ç¨‹é•¿åº¦: {len(thinking_content)} å­—ç¬¦")

        return {
            'attempt': attempt_num,
            'answer': answer,
            'response': full_response,
            'thinking_content': thinking_content,
            'thinking_length': len(thinking_content),
            'duration': duration,
            'thread_id': thread_id,
            'model_name': model_name,
            'comparison_result': comparison_result
        }

    except Exception as e:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡æ±‚è§£æœ€ç»ˆå¤±è´¥({model_name},{mode_tag}): {e}")
        return {
            'attempt': attempt_num,
            'answer': "",
            'response': f"è¯·æ±‚å¤±è´¥: {str(e)}",
            'thinking_content': "",
            'thinking_length': 0,
            'duration': 0,
            'thread_id': thread_id,
            'model_name': model_name,
            'comparison_result': "å¤„ç†å¤±è´¥"
        }


# === [REPLACE] è§£æå‡½æ•°ï¼šå…¼å®¹è‹±æ–‡å­—æ®µå¹¶æå–æœ¬åœ°å›¾ç‰‡è·¯å¾„ ===
def parse_question_data(data):
    """è§£æä¸åŒæ ¼å¼çš„é¢˜ç›®æ•°æ® - å…¼å®¹ {question/image/answer} å¹¶ç”Ÿæˆ å›¾ç‰‡è·¯å¾„"""
    import pandas as pd

    def safe_str_convert(value):
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return ""
        return str(value).strip()
    question_num = (
        data.get("row") or
        data.get("question_id") or
        data.get("é¢˜å·") or
        data.get("id") or
        0
    )


    # é¢˜å¹²
    question_text = safe_str_convert(
        data.get("prompt", "") or data.get("é¢˜å¹²", "") or data.get("question", "")
    )

    # è¿‡ç¨‹ï¼ˆè‹¥è‹±æ–‡æ•°æ®æ— è§£é¢˜è¿‡ç¨‹åˆ™ä¸ºç©ºï¼‰
    process = safe_str_convert(
        data.get("rationale", "") or data.get("è¿‡ç¨‹", "")
    )

    # ç­”æ¡ˆ
    answer = safe_str_convert(
        data.get("answer", "") or data.get("ç­”æ¡ˆ", "")
    )

    # å›¾ç‰‡è·¯å¾„ï¼ˆä» image / rationale_image æå–URL â†’ æœ¬åœ° image/xxx.jpgï¼‰
    image_path = ""
    for k in ("å›¾ç‰‡è·¯å¾„", "image", "rationale_image"):
        image_path = _extract_local_image_path(safe_str_convert(data.get(k, "")))
        if image_path:
            break

    return {
        "é¢˜å·": question_num,
        "é¢˜å¹²": question_text,
        "è¿‡ç¨‹": process,
        "ç­”æ¡ˆ": answer,
        "å›¾ç‰‡è·¯å¾„": image_path
    }



def solve_question_parallel(question_data, models, num_attempts, pass_threshold, max_workers=2,
                            delay_between_requests=1):
    """å¹¶è¡Œæ±‚è§£å•ä¸ªé—®é¢˜ - ä¿®æ­£åˆæ ¼åˆ¤æ–­é€»è¾‘ä¸º <= kï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬åˆ¤æ–­åˆæ ¼"""
    parsed_data = parse_question_data(question_data)
    question_num = parsed_data["é¢˜å·"]
    question_text = parsed_data["é¢˜å¹²"]
    original_process = parsed_data["è¿‡ç¨‹"]
    original_answer = parsed_data["ç­”æ¡ˆ"]
    image_path = parsed_data.get("å›¾ç‰‡è·¯å¾„", "")
    safe_print(f"\n{'=' * 60}")
    safe_print(f"å¼€å§‹å¹¶è¡Œå¤„ç†é¢˜å· {question_num}")
    safe_print(f"é¢˜ç›®: {question_text[:150]}...")
    safe_print(f"ä½¿ç”¨æ¨¡å‹: {', '.join(models)}")
    safe_print(f"æ¯ä¸ªæ¨¡å‹å°è¯•æ¬¡æ•°: {num_attempts}")
    safe_print(f"åˆæ ¼æ ‡å‡†: æ­£ç¡®æ¬¡æ•° <= {pass_threshold}")
    safe_print(f"å¹¶è¡Œçº¿ç¨‹æ•°: {max_workers}")
    if original_process:
        safe_print(f"åŸå§‹è¿‡ç¨‹: {original_process[:100]}...")
    if original_answer:
        safe_print(f"æ ‡å‡†ç­”æ¡ˆ: {original_answer}")
    else:
        safe_print(f"æ— æ ‡å‡†ç­”æ¡ˆï¼Œä¸è¿›è¡Œæ¯”å¯¹")
    safe_print(f"{'=' * 60}")

    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå¤šæ¬¡å°è¯•çš„ä»»åŠ¡
    tasks = []
    for model in models:
        for attempt in range(1, num_attempts + 1):
            tasks.append((question_text, model, attempt, question_num, original_answer, image_path))

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = []
        batch_size = max_workers

        for batch_start in range(0, len(tasks), batch_size):
            batch_end = min(batch_start + batch_size, len(tasks))

            futures = []
            for i in range(batch_start, batch_end):
                question_text, model, attempt, question_num, standard_answer, image_path = tasks[i]
                future = executor.submit(
                    ask_model_for_math_single,
                    question_text, model, attempt, question_num,
                    standard_answer, image_path
                )
                futures.append(future)
                if delay_between_requests > 0 and i < batch_end - 1:
                    time.sleep(delay_between_requests)

            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    safe_print(f"è·å–ç»“æœæ—¶å‡ºé”™: {e}")
                    results.append({
                        'attempt': len(results) + 1,
                        'answer': "",
                        'response': f"å¤„ç†å¤±è´¥: {str(e)}",
                        'thinking_content': "",
                        'thinking_length': 0,
                        'duration': 0,
                        'thread_id': threading.current_thread().ident,
                        'model_name': 'unknown',
                        'comparison_result': "å¤„ç†å¤±è´¥"
                    })

            if batch_end < len(tasks):
                safe_print(f"æ‰¹æ¬¡å®Œæˆï¼Œç­‰å¾… {delay_between_requests * 2} ç§’åç»§ç»­...")
                time.sleep(delay_between_requests * 2)

    # æŒ‰æ¨¡å‹å’Œattemptæ’åº
    results.sort(key=lambda x: (x['model_name'], x['attempt']))
    # 1. æ„å»ºç­”æ¡ˆæ±‡æ€»è¡¨ - æ ¼å¼ï¼šé¢˜å·ã€é¢˜ç›®ã€åŸå§‹è¿‡ç¨‹ã€æ ‡å‡†ç­”æ¡ˆã€å„æ¬¡ç­”æ¡ˆã€æ¯ä¸ªæ¨¡å‹çš„åˆæ ¼çŠ¶æ€
    answer_summary_row = {
        "é¢˜å·": question_num,
        "é¢˜ç›®": question_text,
        "åŸå§‹è¿‡ç¨‹": original_process,
        "æ ‡å‡†ç­”æ¡ˆ": original_answer
    }
    # ä¸ºæ¯ä¸ªæ¨¡å‹çš„æ¯æ¬¡å°è¯•æ·»åŠ ç­”æ¡ˆåˆ—
    for result in results:
        model = result['model_name']
        attempt = result['attempt']
        col_name = f"ç­”æ¡ˆ_{model}_ç¬¬{attempt}æ¬¡"
        answer_summary_row[col_name] = result['answer'] if result['answer'] else "[ç©º]"

    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„åˆæ ¼çŠ¶æ€
    model_qualifications = {}
    if original_answer and original_answer.strip():
        # æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬è®¡ç®—æ­£ç¡®æ¬¡æ•°
        for model in models:
            model_results = [r for r in results if r['model_name'] == model]
            correct_count = sum(1 for r in model_results if r['comparison_result'] == "æ­£ç¡®")
            is_qualified = "åˆæ ¼" if correct_count <= pass_threshold else "ä¸åˆæ ¼"
            qualification_detail = f"{is_qualified}({correct_count}/{len(model_results)})"
            model_qualifications[model] = qualification_detail

            # æ·»åŠ åˆ°ç­”æ¡ˆæ±‡æ€»è¡Œ
            answer_summary_row[f"{model}_åˆæ ¼çŠ¶æ€"] = qualification_detail
    else:
        # æ— æ ‡å‡†ç­”æ¡ˆ
        for model in models:
            model_qualifications[model] = "æ— æ ‡å‡†ç­”æ¡ˆ"
            answer_summary_row[f"{model}_åˆæ ¼çŠ¶æ€"] = "æ— æ ‡å‡†ç­”æ¡ˆ"

    # 2. æ„å»ºç»Ÿè®¡ä¿¡æ¯è¡¨ - åŒ…å«æ€§èƒ½æ•°æ®å’Œæ¯”å¯¹ç»Ÿè®¡
    stats_rows = []
    for result in results:
        stats_rows.append({
            "é¢˜å·": question_num,
            "æ¨¡å‹": result['model_name'],
            "å°è¯•æ¬¡æ•°": result['attempt'],
            "è€—æ—¶(ç§’)": round(result['duration'], 2),
            "æ€è€ƒé•¿åº¦": result['thinking_length'],
            "çº¿ç¨‹ID": result.get('thread_id', 'unknown'),
            "ç­”æ¡ˆ": result['answer'] if result['answer'] else "[ç©º-éœ€äººå·¥æå–]",
            "æ¯”å¯¹ç»“æœ": result['comparison_result']
        })

    # 3. æ„å»ºè§£é¢˜è¿‡ç¨‹è¡¨ - å®Œæ•´çš„è§£é¢˜å’Œæ€è€ƒå†…å®¹
    process_rows = []
    for result in results:
        process_rows.append({
            "é¢˜å·": question_num,
            "é¢˜ç›®": question_text,
            "æ¨¡å‹": result['model_name'],
            "å°è¯•æ¬¡æ•°": result['attempt'],
            "ç­”æ¡ˆ": result['answer'] if result['answer'] else "[ç©º-éœ€äººå·¥æå–]",
            "æ¯”å¯¹ç»“æœ": result['comparison_result'],
            "è§£é¢˜è¿‡ç¨‹": result['response'],
            "æ€è€ƒè¿‡ç¨‹": result['thinking_content'],
            "è€—æ—¶(ç§’)": round(result['duration'], 2),
            "æ€è€ƒé•¿åº¦": result['thinking_length'],
            "çº¿ç¨‹ID": result.get('thread_id', 'unknown')
        })

    # ç»Ÿè®¡æ¯”å¯¹ç»“æœ
    if original_answer and original_answer.strip():
        safe_print(f"\né¢˜å· {question_num} å„æ¨¡å‹åˆæ ¼çŠ¶æ€:")
        for model, qualification in model_qualifications.items():
            safe_print(f"  {model}: {qualification}")
    else:
        safe_print(f"\né¢˜å· {question_num} æ— æ ‡å‡†ç­”æ¡ˆï¼Œè·³è¿‡æ¯”å¯¹ç»Ÿè®¡")

    safe_print(f"\né¢˜å· {question_num} å®Œæˆæ‰€æœ‰æ±‚è§£:")
    for result in results:
        model = result['model_name']
        attempt = result['attempt']
        answer_display = result['answer'] if result['answer'] and result['answer'] != "[ç©º]" else "[ç©º-éœ€äººå·¥æå–]"
        comparison_display = result['comparison_result']
        safe_print(
            f"  {model}_ç¬¬{attempt}æ¬¡: {answer_display} (è€—æ—¶: {result['duration']:.2f}s, æ€è€ƒ: {result['thinking_length']}å­—ç¬¦, æ¯”å¯¹: {comparison_display})")
    return answer_summary_row, stats_rows, process_rows


def calculate_overall_pass_rate(answer_summary_results, pass_threshold, models):
    """è®¡ç®—æ•´ä½“åˆæ ¼ç‡ç»Ÿè®¡ - åªåŒ…å«æ¯ä¸ªæ¨¡å‹çš„ç»Ÿè®¡"""
    if not answer_summary_results:
        return {}

    total_questions = len(answer_summary_results)

    # æ¯ä¸ªæ¨¡å‹çš„ç»Ÿè®¡
    model_stats = {}
    for model in models:
        model_stats[model] = {
            'passed': 0,
            'failed': 0,
            'no_standard': 0
        }

    question_details = []

    for row in answer_summary_results:
        question_num = row['é¢˜å·']

        # æ¯ä¸ªæ¨¡å‹çš„ç»Ÿè®¡
        model_statuses = {}
        for model in models:
            model_qual_key = f"{model}_åˆæ ¼çŠ¶æ€"
            if model_qual_key in row:
                model_qualification = row[model_qual_key]
                if model_qualification == "æ— æ ‡å‡†ç­”æ¡ˆ":
                    model_stats[model]['no_standard'] += 1
                    model_statuses[model] = "æ— æ ‡å‡†ç­”æ¡ˆ"
                elif "åˆæ ¼" in model_qualification:
                    model_stats[model]['passed'] += 1
                    model_statuses[model] = "åˆæ ¼"
                else:
                    model_stats[model]['failed'] += 1
                    model_statuses[model] = "ä¸åˆæ ¼"

        question_detail = {
            'é¢˜å·': question_num
        }

        # æ·»åŠ æ¯ä¸ªæ¨¡å‹çš„çŠ¶æ€
        for model in models:
            question_detail[f'{model}_çŠ¶æ€'] = model_statuses.get(model, "æœªçŸ¥")

        question_details.append(question_detail)

    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„åˆæ ¼ç‡
    model_pass_rates = {}
    for model in models:
        model_with_standard = model_stats[model]['passed'] + model_stats[model]['failed']
        if model_with_standard > 0:
            model_pass_rates[model] = f"{model_stats[model]['passed'] / model_with_standard * 100:.1f}%"
        else:
            model_pass_rates[model] = "N/A (æ— æ ‡å‡†ç­”æ¡ˆ)"

    return {
        'total_questions': total_questions,
        'model_stats': model_stats,
        'model_pass_rates': model_pass_rates,
        'question_details': question_details,
        'pass_threshold': pass_threshold
    }

# === [ADD] è§†è§‰è°ƒç”¨ç‰ˆæœ¬ï¼šæŠŠå›¾ç‰‡ä¸æ–‡æœ¬ä¸€èµ·å‘ç»™æ¨¡å‹ ===
def call_model_with_retry_vision(question, model_name, attempt_num, question_num, image_path):
    client = get_openai_client()

    math_prompt = f"""è¯·è§£å†³ä»¥ä¸‹æ•°å­¦/é€»è¾‘é¢˜ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨\\boxed{{}}ä¸­ï¼š
è‹¥å›¾ç‰‡ä¸­åŒ…å«å…³é”®ä¿¡æ¯ï¼Œè¯·ç»“åˆå›¾ç‰‡ä¸æ–‡å­—ä¸€å¹¶ä½œç­”ã€‚

é¢˜ç›®ï¼š
{question}

è¯·æä¾›æ¸…æ™°æ­¥éª¤ï¼Œå¹¶åœ¨æœ€åç”¨\\boxed{{}}æ ‡å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"""

    start_time = time.time()
    safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - [VISION] è°ƒç”¨ {model_name}ï¼Œå›¾ç‰‡ï¼š{image_path}")

    # æ„é€ å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ–‡æœ¬ + image_url dataURIï¼‰
    data_url = _build_data_url_with_pillow(image_path)
    messages = [{
        "role": "user",
        "content": [
            {"type": "text", "text": math_prompt},
            {"type": "image_url", "image_url": {"url": data_url, "detail": "high"}}
        ]
    }]

    # å¤§å¤šæ¨¡å‹éƒ½èƒ½ç”¨æ ‡å‡† chat.completionsï¼›
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        stream=False
    )
    full_response = (response.choices[0].message.content or "")
    thinking_content = ""
    duration = time.time() - start_time

    if duration < 2:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - è­¦å‘Šï¼šè°ƒç”¨æ—¶é—´è¿‡çŸ­({duration:.2f}ç§’)")
    if len(full_response.strip()) < 50:
        safe_print(f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - è­¦å‘Šï¼šå“åº”å†…å®¹è¿‡çŸ­({len(full_response)}å­—ç¬¦)")
        raise Exception("å“åº”å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯APIè°ƒç”¨å¼‚å¸¸")

    safe_print(
        f"é¢˜å·{question_num} ç¬¬{attempt_num}æ¬¡ - [VISION]{model_name} è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’ï¼Œå“åº”é•¿åº¦: {len(full_response)}å­—ç¬¦")
    return {
        'full_response': full_response.strip(),
        'thinking_content': thinking_content,
        'duration': duration,
        'model_name': model_name
    }


def select_models():
    """é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ - æ”¯æŒè‡ªå®šä¹‰è¾“å…¥"""
    print("\nå¸¸ç”¨æ¨¡å‹å‚è€ƒåˆ—è¡¨:")
    for i, model in enumerate(COMMON_MODELS, 1):
        print(f"{i:2d}. {model}")

    print("\nè¯·è¾“å…¥è¦ä½¿ç”¨çš„æ¨¡å‹åç§°:")
    print("- å¯ä»¥ç›´æ¥è¾“å…¥æ¨¡å‹åç§°ï¼Œå¤šä¸ªæ¨¡å‹ç”¨é€—å·åˆ†éš”")
    print("- ä¹Ÿå¯ä»¥è¾“å…¥ä¸Šé¢åˆ—è¡¨ä¸­çš„æ•°å­—ï¼Œå¤šä¸ªæ•°å­—ç”¨é€—å·åˆ†éš”")
    print("- ä¾‹å¦‚: Qwen3-235B-A22B,gpt-4o-0806-global")
    print("- æˆ–è€…: 1,2,3")

    selection = input("\nè¾“å…¥é€‰æ‹©: ").strip()

    if not selection:
        print("æœªè¾“å…¥ä»»ä½•å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: Qwen3-235B-A22B")
        return ["Qwen3-235B-A22B"]

    selected_models = []

    # æŒ‰é€—å·åˆ†å‰²è¾“å…¥
    choices = [choice.strip() for choice in selection.split(',')]

    for choice in choices:
        if choice.isdigit():
            # å¦‚æœæ˜¯æ•°å­—ï¼Œä»å¸¸ç”¨æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©
            index = int(choice) - 1
            if 0 <= index < len(COMMON_MODELS):
                model_name = COMMON_MODELS[index]
                if model_name not in selected_models:
                    selected_models.append(model_name)
                    print(f"âœ“ å·²é€‰æ‹©: {model_name}")
            else:
                print(f"âœ— æ— æ•ˆæ•°å­—: {choice} (è¶…å‡ºèŒƒå›´)")
        else:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½œä¸ºæ¨¡å‹åç§°
            if choice and choice not in selected_models:
                selected_models.append(choice)
                print(f"âœ“ å·²é€‰æ‹©: {choice}")

    if not selected_models:
        print("æœªé€‰æ‹©ä»»ä½•æœ‰æ•ˆæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: Qwen3-235B-A22B")
        selected_models = ["Qwen3-235B-A22B"]

    print(f"\næœ€ç»ˆé€‰æ‹©çš„æ¨¡å‹: {', '.join(selected_models)}")

    # ç¡®è®¤é€‰æ‹©
    confirm = input("ç¡®è®¤ä½¿ç”¨è¿™äº›æ¨¡å‹ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("é‡æ–°é€‰æ‹©æ¨¡å‹...")
        return select_models()

    return selected_models


def process_math_questions_parallel(input_file, output_file, models, num_attempts, pass_threshold, start_index=0,
                                    max_workers=2, delay_between_requests=1):
    """å¹¶è¡Œå¤„ç†æ•°å­¦é—®é¢˜ - ä¿®æ­£åˆæ ¼åˆ¤æ–­é€»è¾‘ä¸º <= kï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬åˆ¤æ–­åˆæ ¼"""

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    questions_data = []
    for line in lines:
        line = line.strip()
        if line:
            try:
                data = json.loads(line)
                questions_data.append(data)
            except json.JSONDecodeError as e:
                print(f"è§£æJSONå‡ºé”™: {e}, è¡Œå†…å®¹: {line}")
                continue
    for idx, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
        try:
            raw = json.loads(line)
        except json.JSONDecodeError as e:
            print(f"è§£æJSONå‡ºé”™: {e}, è¡Œå†…å®¹: {line}")
            continue

        # === [ADD] è‹¥æ—  row/é¢˜å·ï¼Œåˆ™ç”¨è¡Œå·ç”Ÿæˆï¼ˆ1-basedï¼‰
        if not raw.get("row") and not raw.get("é¢˜å·"):
            raw["row"] = idx + 1
    questions_to_process = questions_data[start_index:]

    print(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(questions_to_process)} ä¸ªé—®é¢˜")
    print(f"ä½¿ç”¨æ¨¡å‹: {', '.join(models)}")
    print(f"æ¯ä¸ªæ¨¡å‹å°è¯•æ¬¡æ•°: {num_attempts}")
    print(f"åˆæ ¼æ ‡å‡†: æ­£ç¡®æ¬¡æ•° <= {pass_threshold}")
    print(f"å¹¶è¡Œçº¿ç¨‹æ•°: {max_workers}")
    print(f"è¯·æ±‚é—´éš”: {delay_between_requests}ç§’")

    # åˆ†åˆ«æ”¶é›†ä¸‰ç§ç±»å‹çš„æ•°æ®
    answer_summary_results = []  # ç­”æ¡ˆæ±‡æ€»è¡¨æ•°æ®
    all_stats_rows = []  # ç»Ÿè®¡ä¿¡æ¯è¡¨æ•°æ®
    all_process_rows = []  # è§£é¢˜è¿‡ç¨‹è¡¨æ•°æ®

    for i, question_data in enumerate(questions_to_process):
        print(f"\nå¤„ç†è¿›åº¦: {i + 1}/{len(questions_to_process)}")

        # è·å–ä¸‰ç§ç±»å‹çš„æ•°æ®
        answer_summary_row, stats_rows, process_rows = solve_question_parallel(
            question_data, models, num_attempts, pass_threshold, max_workers, delay_between_requests
        )

        # åˆ†åˆ«æ·»åŠ åˆ°å¯¹åº”çš„åˆ—è¡¨
        answer_summary_results.append(answer_summary_row)
        all_stats_rows.extend(stats_rows)
        all_process_rows.extend(process_rows)

        # å®æ—¶ä¿å­˜ - åˆ›å»ºä¸‰ä¸ªå·¥ä½œè¡¨
        df_answer_summary = pd.DataFrame(answer_summary_results)
        df_stats = pd.DataFrame(all_stats_rows)
        df_process = pd.DataFrame(all_process_rows)

        # è®¡ç®—æ•´ä½“åˆæ ¼ç‡ç»Ÿè®¡
        overall_stats = calculate_overall_pass_rate(answer_summary_results, pass_threshold, models)
        df_overall_stats = pd.DataFrame(overall_stats.get('question_details', []))

        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # å·¥ä½œè¡¨1: ç­”æ¡ˆæ±‡æ€» - åŒ…å«æ¯ä¸ªæ¨¡å‹çš„åˆæ ¼çŠ¶æ€
            df_answer_summary.to_excel(writer, sheet_name='ç­”æ¡ˆæ±‡æ€»', index=False)

            # å·¥ä½œè¡¨2: ç»Ÿè®¡ä¿¡æ¯ - æ€§èƒ½åˆ†ææ•°æ®
            df_stats.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)

            # å·¥ä½œè¡¨3: è§£é¢˜è¿‡ç¨‹ - å®Œæ•´çš„è§£é¢˜å’Œæ€è€ƒå†…å®¹
            df_process.to_excel(writer, sheet_name='è§£é¢˜è¿‡ç¨‹', index=False)

            # å·¥ä½œè¡¨4: æ¨¡å‹ç»Ÿè®¡ (å¦‚æœæœ‰æ•°æ®)
            if not df_overall_stats.empty:
                df_overall_stats.to_excel(writer, sheet_name='æ¨¡å‹ç»Ÿè®¡', index=False)

        print(f"å·²å®Œæˆ {i + 1}/{len(questions_to_process)} ä¸ªé—®é¢˜ï¼Œç»“æœå·²ä¿å­˜")

        # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
        if overall_stats:
            print("å„æ¨¡å‹åˆæ ¼ç‡:")
            for model in models:
                model_rate = overall_stats['model_pass_rates'].get(model, "N/A")
                model_stat = overall_stats['model_stats'].get(model, {})
                passed = model_stat.get('passed', 0)
                total = passed + model_stat.get('failed', 0)
                print(f"  {model}: {model_rate} ({passed}/{total})")

        if i < len(questions_to_process) - 1:
            print(f"ç­‰å¾… {delay_between_requests * 3} ç§’åå¤„ç†ä¸‹ä¸€é¢˜...")
            time.sleep(delay_between_requests * 3)

    print(f"\næ‰€æœ‰é—®é¢˜å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

    # æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š
    final_stats = calculate_overall_pass_rate(answer_summary_results, pass_threshold, models)
    if final_stats:
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š:")
        print(f"æ€»é¢˜ç›®æ•°: {final_stats['total_questions']}")
        print(f"åˆæ ¼æ ‡å‡†: æ­£ç¡®æ¬¡æ•° <= {pass_threshold}")

        print(f"\nå„æ¨¡å‹è¯¦ç»†ç»Ÿè®¡:")
        for model in models:
            model_rate = final_stats['model_pass_rates'].get(model, "N/A")
            model_stat = final_stats['model_stats'].get(model, {})
            passed = model_stat.get('passed', 0)
            failed = model_stat.get('failed', 0)
            no_standard = model_stat.get('no_standard', 0)
            total_with_standard = passed + failed
            print(f"  {model}:")
            print(f"    åˆæ ¼ç‡: {model_rate} ({passed}/{total_with_standard})")
            print(f"    åˆæ ¼: {passed}, ä¸åˆæ ¼: {failed}, æ— æ ‡å‡†ç­”æ¡ˆ: {no_standard}")

    print("\nè¡¨æ ¼ç»“æ„è¯´æ˜:")
    print("ğŸ“Š ç­”æ¡ˆæ±‡æ€»è¡¨: é¢˜å·ã€é¢˜ç›®ã€åŸå§‹è¿‡ç¨‹ã€æ ‡å‡†ç­”æ¡ˆã€å„æ¬¡ç­”æ¡ˆã€å„æ¨¡å‹åˆæ ¼çŠ¶æ€")
    print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯è¡¨: é¢˜å·ã€æ¨¡å‹ã€å°è¯•æ¬¡æ•°ã€è€—æ—¶ã€æ€è€ƒé•¿åº¦ã€ç­”æ¡ˆã€æ¯”å¯¹ç»“æœ")
    print("ğŸ“ è§£é¢˜è¿‡ç¨‹è¡¨: å®Œæ•´çš„è§£é¢˜è¿‡ç¨‹å’Œæ€è€ƒå†…å®¹")
    print("ğŸ“‹ æ¨¡å‹ç»Ÿè®¡è¡¨: æ¯é¢˜å„æ¨¡å‹çš„çŠ¶æ€æ±‡æ€»")
    print("\næ³¨æ„ï¼š")
    print("- æ ‡è®°ä¸º[ç©º]çš„ç­”æ¡ˆéœ€è¦æ ¹æ®è§£é¢˜è¿‡ç¨‹äººå·¥æå–")
    print("- åªæœ‰åŒ…å«æ ‡å‡†ç­”æ¡ˆçš„é¢˜ç›®æ‰å‚ä¸åˆæ ¼åˆ¤æ–­")
    print("- æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆçš„é¢˜ç›®æ˜¾ç¤º'æ— æ ‡å‡†ç­”æ¡ˆ'")
    print("- åˆæ ¼æ ‡å‡†ï¼šæ­£ç¡®æ¬¡æ•° <= k (é”™è¯¯æ¬¡æ•°è¾ƒå°‘æ‰åˆæ ¼)")
    print("- æ¯ä¸ªæ¨¡å‹å•ç‹¬åˆ¤æ–­åˆæ ¼")
    return answer_summary_results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("æ•°å­¦é¢˜æ±‚è§£å™¨ - å¤šæ¨¡å‹å¹¶è¡Œç‰ˆæœ¬ (åˆæ ¼æ ‡å‡†: æ­£ç¡®æ¬¡æ•° <= k)")
    print("æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹è¾“å…¥å’Œå¤šç§JSONLæ ¼å¼")
    print("ç­”æ¡ˆæ±‡æ€»è¡¨æ ¼å¼: é¢˜å·ã€é¢˜ç›®ã€åŸå§‹è¿‡ç¨‹ã€æ ‡å‡†ç­”æ¡ˆã€å„æ¬¡ç­”æ¡ˆã€å„æ¨¡å‹åˆæ ¼çŠ¶æ€")
    print("=" * 60)

    choice = input("é€‰æ‹©æ¨¡å¼:\n1. æµ‹è¯•å•é¢˜\n2. å¤„ç†å®Œæ•´æ–‡ä»¶\nè¯·è¾“å…¥é€‰æ‹© (1/2): ")

    if choice == "1":
        # æµ‹è¯•å•é¢˜
        models = select_models()

        num_attempts = input("æ¯ä¸ªæ¨¡å‹å°è¯•å‡ æ¬¡ï¼Ÿ(é»˜è®¤3): ")
        try:
            num_attempts = int(num_attempts) if num_attempts.strip() else 3
        except ValueError:
            num_attempts = 3

        pass_threshold = input(f"åˆæ ¼æ ‡å‡†ï¼šæ­£ç¡®æ¬¡æ•°æœ€å¤šå‡ æ¬¡ï¼Ÿ(é»˜è®¤1ï¼Œå¿…é¡»å°äº{num_attempts}): ")
        try:
            pass_threshold = int(pass_threshold) if pass_threshold.strip() else 1
            if pass_threshold >= num_attempts:
                print(
                    f"è­¦å‘Šï¼šåˆæ ¼æ ‡å‡†({pass_threshold})ä¸èƒ½å¤§äºç­‰äºå°è¯•æ¬¡æ•°({num_attempts})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º{num_attempts - 1}")
                pass_threshold = num_attempts - 1
        except ValueError:
            pass_threshold = 1

        max_workers = input("è¾“å…¥å¹¶è¡Œçº¿ç¨‹æ•° (æ¨è4-8): ")
        try:
            max_workers = int(max_workers) if max_workers.strip() else 4
        except ValueError:
            max_workers = 4

        delay = input("è¾“å…¥è¯·æ±‚é—´éš”ç§’æ•° (æ¨è2-3): ")
        try:
            delay = float(delay) if delay.strip() else 2
        except ValueError:
            delay = 2

        # æµ‹è¯•é¢˜ç›® - åŒ…å«æ ‡å‡†ç­”æ¡ˆ
        test_question = {
            "é¢˜å·": 1,
            "é¢˜å¹²": "Find the singular values of the matrix $ A = \\begin{bmatrix} \\sqrt{3} & 2 & -\\sqrt{2} & 1 & 0 \\\\ 1 & \\sqrt{5} & 0 & -1 & 3 \\\\ -2 & 1 & 2\\sqrt{2} & \\sqrt{3} & -1 \\\\ 0 & -3 & 1 & \\sqrt{7} & 2 \\\\ 4 & 0 & \\sqrt{6} & -2 & \\sqrt{10} \\end{bmatrix} $.",
            "ç­”æ¡ˆ": "2âˆš6, âˆš19, âˆš17, 4, 2âˆš6"  # ç¤ºä¾‹æ ‡å‡†ç­”æ¡ˆ
        }

        # è·å–ä¸‰ç§ç±»å‹çš„æ•°æ®
        answer_summary_row, stats_rows, process_rows = solve_question_parallel(
            test_question, models, num_attempts, pass_threshold, max_workers, delay
        )

        # ä¿å­˜ç»“æœåˆ°å·¥ä½œè¡¨
        df_answer_summary = pd.DataFrame([answer_summary_row])
        df_stats = pd.DataFrame(stats_rows)
        df_process = pd.DataFrame(process_rows)
        output_file = "test_multi_model.xlsx"

        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            df_answer_summary.to_excel(writer, sheet_name='ç­”æ¡ˆæ±‡æ€»', index=False)
            df_stats.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
            df_process.to_excel(writer, sheet_name='è§£é¢˜è¿‡ç¨‹', index=False)

        print(f"\næµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("\nè¡¨æ ¼ç»“æ„è¯´æ˜:")
        print("ğŸ“Š ç­”æ¡ˆæ±‡æ€»è¡¨: é¢˜å·ã€é¢˜ç›®ã€åŸå§‹è¿‡ç¨‹ã€æ ‡å‡†ç­”æ¡ˆã€å„æ¬¡ç­”æ¡ˆã€å„æ¨¡å‹åˆæ ¼çŠ¶æ€")
        print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯è¡¨: é¢˜å·ã€æ¨¡å‹ã€å°è¯•æ¬¡æ•°ã€è€—æ—¶ã€æ€è€ƒé•¿åº¦ã€ç­”æ¡ˆã€æ¯”å¯¹ç»“æœ")
        print("ğŸ“ è§£é¢˜è¿‡ç¨‹è¡¨: å®Œæ•´çš„è§£é¢˜è¿‡ç¨‹å’Œæ€è€ƒå†…å®¹")

    elif choice == "2":
        # å¤„ç†å®Œæ•´æ–‡ä»¶
        input_json = input("è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„ (é»˜è®¤: é¢˜ç›®.jsonl): ").strip()
        if not input_json:
            input_json = "é¢˜ç›®.jsonl"

        output_excel = input("è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„ (é»˜è®¤: é¢˜ç›®ç­”æ¡ˆ.xlsx): ").strip()
        if not output_excel:
            output_excel = "é¢˜ç›®ç­”æ¡ˆ.xlsx"

        if os.path.exists(input_json):
            models = select_models()

            num_attempts = input("æ¯ä¸ªæ¨¡å‹å°è¯•å‡ æ¬¡ï¼Ÿ(é»˜è®¤3): ")
            try:
                num_attempts = int(num_attempts) if num_attempts.strip() else 3
            except ValueError:
                num_attempts = 3

            pass_threshold = input(f"åˆæ ¼æ ‡å‡†ï¼šæ­£ç¡®æ¬¡æ•°æœ€å¤šå‡ æ¬¡ï¼Ÿ(é»˜è®¤1ï¼Œå¿…é¡»å°äº{num_attempts}): ")
            try:
                pass_threshold = int(pass_threshold) if pass_threshold.strip() else 1
                if pass_threshold >= num_attempts:
                    print(
                        f"è­¦å‘Šï¼šåˆæ ¼æ ‡å‡†({pass_threshold})ä¸èƒ½å¤§äºç­‰äºå°è¯•æ¬¡æ•°({num_attempts})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º{num_attempts - 1}")
                    pass_threshold = num_attempts - 1
            except ValueError:
                pass_threshold = 1

            max_workers = input("è¾“å…¥å¹¶è¡Œçº¿ç¨‹æ•° (æ¨è4-8): ")
            try:
                max_workers = int(max_workers) if max_workers.strip() else 4
            except ValueError:
                max_workers = 4

            delay = input("è¾“å…¥è¯·æ±‚é—´éš”ç§’æ•° (æ¨è2-3): ")
            try:
                delay = float(delay) if delay.strip() else 2
            except ValueError:
                delay = 2

            start_index = input("ä»ç¬¬å‡ ä¸ªé—®é¢˜å¼€å§‹ï¼Ÿ(é»˜è®¤0): ")
            try:
                start_index = int(start_index) if start_index.strip() else 0
            except ValueError:
                start_index = 0

            print(f"\né…ç½®ç¡®è®¤:")
            print(f"æ¨¡å‹: {', '.join(models)}")
            print(f"æ¯ä¸ªæ¨¡å‹å°è¯•æ¬¡æ•°: {num_attempts}")
            print(f"åˆæ ¼æ ‡å‡†: æ­£ç¡®æ¬¡æ•° <= {pass_threshold}")
            print(f"å¹¶è¡Œçº¿ç¨‹æ•°: {max_workers}")
            print(f"è¯·æ±‚é—´éš”: {delay}ç§’")
            print(f"å¼€å§‹ä½ç½®: {start_index}")

            confirm = input("ç¡®è®¤å¼€å§‹å¤„ç†ï¼Ÿ(y/n): ")
            if confirm.lower() == 'y':
                process_math_questions_parallel(
                    input_json, output_excel, models, num_attempts, pass_threshold,
                    start_index, max_workers, delay
                )
            else:
                print("å·²å–æ¶ˆå¤„ç†")
        else:
            print(f"è¾“å…¥æ–‡ä»¶ {input_json} ä¸å­˜åœ¨")
    else:
        print("æ— æ•ˆé€‰æ‹©")
